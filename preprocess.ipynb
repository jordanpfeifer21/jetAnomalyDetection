{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from format_data import format_2D\n",
    "from models import Autoencoder, train_model, Transformer, SmallAutoencoder\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "from data_analysis import plot_property_distribution, plot_property_distribution2\n",
    "# from torchsummary import summary\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import constants as c\n",
    "import pandas as pd \n",
    "from model_analysis import plot_loss, plot_anomaly_score, plot_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_label = \"QCD\"\n",
    "signal_label = \"WJet\"\n",
    "props = ['dzErr', 'pt']\n",
    "data_dir = \"/isilon/data/users/jpfeife2/AutoEncoder-Anomaly-Detection/processed_data\"\n",
    "\n",
    "batch_size = 20\n",
    "epochs = 10\n",
    "initial_lr = 0.001\n",
    "weight_decay = 1e-3\n",
    "latent_dim = 12\n",
    "\n",
    "n_props = len(props)\n",
    "# n_props = 8\n",
    "# props = ['-211', '-13', '-11', '11', '13', '22', '130', '211']\n",
    "\n",
    "prop_string = ''.join(['_' + str(prop) for prop in props])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_file = data_dir + \"/background.pkl\"\n",
    "signal_file = data_dir + \"/signal.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "background_ = pd.read_pickle(background_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_ = pd.read_pickle(signal_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from format_data import one_hot_encode_list, make_histogram\n",
    "import pandas as pd \n",
    "from data_analysis import make_histogram\n",
    "import numpy as np \n",
    "# import dgl #deep graph network that will make integrating graphs easier\n",
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "import constants as c\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_2D(data, properties, scalers=None):\n",
    "    hists = []\n",
    "    n_properties = len(properties)\n",
    "    if scalers == None: \n",
    "        scalers = []\n",
    "    vals = []\n",
    "\n",
    "    for i, prop in enumerate(properties): \n",
    "        vals.append([])\n",
    "\n",
    "        if prop == 'pdgId': \n",
    "            flattened_list = [item for sublist in data['pdgId'] for item in sublist]\n",
    "            unique_values_list = sorted(list(set(flattened_list)))\n",
    "            # value_to_index = {value: index + 1 for index, value in enumerate(unique_values_list)}\n",
    "            one_hot_dict = one_hot_encode_list(unique_values_list, data['pdgId'])\n",
    "            particle_ids = []\n",
    "            for key in one_hot_dict: \n",
    "                one_hot_list = one_hot_dict[key]\n",
    "                # hists.append([make_histogram(data['eta'][i], data['phi'][i],\n",
    "                #                     [value_to_index[number] for number in one_hot_list[i]]) for i in range(data.shape[0])])\n",
    "                hists.append([make_histogram(data['eta'][j], data['phi'][j], one_hot_list[j]) for j in range(data.shape[0])])\n",
    "                particle_ids.append(key)\n",
    "            n_properties += len(hists) - 1\n",
    "        elif prop[-3:] == \"Err\": \n",
    "            '''\n",
    "            currently only makes sense for dz and d0 because they have \n",
    "            '''\n",
    "            prop1 = prop[:2]\n",
    "            valid_pdg = [-11, 11, -13, 13, -211, 211]\n",
    "            all_vals = [p / z for sublist1, sublist2, sublist3 in zip(data[prop1], data[prop], data['pdgId']) for p, z, x in zip(sublist1, sublist2, sublist3) if z >=0 and x in valid_pdg and np.abs(p/z) <= 5.0]\n",
    "            \n",
    "            if scalers == []:\n",
    "                scaler = preprocessing.StandardScaler().fit(np.array(all_vals).reshape(-1,1))\n",
    "                scalers.append(scaler)\n",
    "\n",
    "            hist_list = []\n",
    "            for j in range(data.shape[0]): \n",
    "                prop_data = pd.to_numeric(data[prop1][j])/pd.to_numeric(data[prop][j])\n",
    "                valid_indices = np.array(data[prop][j]) >= 0 \n",
    "                \n",
    "                # checks that the error is greater than 0 and that it is within 5sigma of the original distribution\n",
    "                prop_data = np.array(prop_data)[valid_indices]\n",
    "                within5sigmaindices = np.where(np.abs(prop_data) <= 5.0)\n",
    "                prop_data = prop_data[within5sigmaindices]\n",
    "                if len(prop_data) > 0: \n",
    "                \n",
    "                    prop_scaled = scalers[i].transform(prop_data.reshape(-1,1))\n",
    "                    filtered_eta = np.array(data['eta'][j])[valid_indices][within5sigmaindices].flatten()\n",
    "                    filtered_phi = np.array(data['phi'][j])[valid_indices][within5sigmaindices].flatten()\n",
    "                    filtered_prop = np.array(prop_scaled).flatten()\n",
    "                    vals[i].extend(filtered_prop)\n",
    "                    hist_list.append(make_histogram(filtered_eta, filtered_phi, filtered_prop))\n",
    "                \n",
    "                else: \n",
    "                    print('error, length of data is ', len(prop_data))\n",
    "                    hist_list.append(make_histogram([0], [0], [0]))\n",
    "            hists.append(hist_list)\n",
    "\n",
    "        else: \n",
    "            flattened_list = [item*500 for sublist in data[prop] for item in sublist]\n",
    "            unique_values_list = sorted(list(set(flattened_list)))\n",
    "            if len(scalers) < n_properties: \n",
    "                scaler = preprocessing.StandardScaler(with_mean=False).fit(np.array(flattened_list).reshape(-1,1))\n",
    "                scalers.append(scaler)\n",
    "            if 0.0 in unique_values_list: \n",
    "                print('warning, 0 in data will cause incorrect data translation to histograms')\n",
    "            hists.append([make_histogram(data['eta'][j], data['phi'][j], scalers[i].transform(np.array(data[prop][j]).reshape(-1,1)).flatten()*300) for j in range(data.shape[0])])\n",
    "            vals[i].extend(flattened_list)\n",
    "            \n",
    "\n",
    "            # print(\"warning: multiplying by 10\")\n",
    "    total_hist = np.stack((hists), axis=-1)\n",
    "    total_hist = np.reshape(total_hist, (-1, c.BINS, c.BINS, n_properties)).astype('float32')\n",
    "\n",
    "    print(\"Length of data: \", len(total_hist))\n",
    "    return total_hist, scalers, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'background_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m background, scalers, background_data \u001b[38;5;241m=\u001b[39m format_2D(\u001b[43mbackground_\u001b[49m, properties\u001b[38;5;241m=\u001b[39mprops, scalers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39msave(data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/background\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prop_string \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, background)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBACKGROUND LOADED AND SAVED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'background_' is not defined"
     ]
    }
   ],
   "source": [
    "background, scalers, background_data = format_2D(background_, properties=props, scalers=None)\n",
    "np.save(data_dir + \"/background\" + prop_string + \".npy\", background)\n",
    "print(\"BACKGROUND LOADED AND SAVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error, length of data is  0\n",
      "error, length of data is  0\n",
      "error, length of data is  0\n",
      "error, length of data is  0\n",
      "error, length of data is  0\n",
      "Length of data:  17896\n",
      "SIGNAL LOADED AND SAVED\n"
     ]
    }
   ],
   "source": [
    "signal, _, signal_data = format_2D(signal_, properties=props, scalers=scalers)\n",
    "np.save(data_dir + \"/signal\" + prop_string + \".npy\", signal)\n",
    "print(\"SIGNAL LOADED AND SAVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, prop in enumerate(props): \n",
    "    plot_property_distribution2(background_data[i], signal_data[i], prop, background_label, signal_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000006334663487\n",
      "757.3946116746873\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([i * i for i in background_data[0]]))\n",
    "print(np.mean([i * i for i in background_data[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1015872597764698\n",
      "852.764850994828\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([i * i for i in signal_data[0]]))\n",
    "print(np.mean([i * i for i in signal_data[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = [i for i in signal_data[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.mean(pt)\n",
    "std = np.std(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = [i for i in pt if (i - m) <= 3 * std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1015872597764698\n",
      "1.0561193452958901\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([i * i for i in signal_data[0]]))\n",
    "print(np.mean([i * i * 30*30/ (500 * 500) for i in pt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
